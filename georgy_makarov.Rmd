---
title: "Predicting sales volume from time and price"
author: "Georgy Makarov"
date: "July 20, 2020"
output: 
    html_document:
        toc: yes
        toc_float: true
        theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

## Executive summary

The goal of this project is to create a shiny web application that could predict
sales volume of a certain product from historic sales data and forecasted price.
Prediction like this could be useful for marketing campaigns in FMCG companies.
The result of the project is practically implemented in brewing company.

The application works on **ARIMAX** model. **Time series** of the model is the 
mean of weekly sales volumes in retail shops. **External regressor** is the mean 
sales price of a product at a given week. Forecasting period is 52 weeks. The 
application predicts sales volume for each product. A user can also limit the 
number of retail shops to include into the model.

The application is built using `R Shiny` and `forecast` library. The model 
showed **91%** of *RMSE*. It generates new prediction in under **20ms** after
the change in price forecast. Forecast visualization uses `ggplot2` package. 
Visualization shows *4-D* graph with actual and forecasted sales volumes, 
corresponding prices and revenue. Final outputs show sums for forecasted period.

## Data description

Dataset of this project is a `.csv` file that contains weekly sales data for
2019. Initial dataset contains **6** columns:  
- SalesItem = quantity of goods sold, pcs.;  
- valueSales = revenue from goods sold, thousands currency;  
- volumeSales = volume of goods sold, litres;  
- Date_Id = week identifier;  
- Shop_Id = retail shop identifier;  
- SKU_Id = good identifier;

## Prerequisite libraries

```{r prerequisite, echo=TRUE}
library(dplyr)
library(lubridate)
library(splines)
library(ggplot2)
library(forecast)
```

## Data manipulation

### Getting data

```{r setwd}
setwd("/home/georgy/Документы/Rstudio")
```

The data contains **7.5M** rows and **6** variables. By default, the date column
comes in as *integer* values. Quantity of goods in *pcs* shows up as a factor
with **13802** levels. Volumes and revenues are numeric. The price is not
present in the dataset.

```{r get_data, echo=TRUE}
mydata <- read.csv(file = "data.csv")
str(mydata)
```

Transforming `Date_Id` to `date` format provides good basis for monthly
visualization. Presenting `SalesItem` in numeric format benefits to building 
model - if quantity in *pcs* is required. Price is an artificial column, which
we form from dividing revenue by volume:

$$ price = \frac{sales}{volume} $$

There are rows with missing volumes. They are practically useless because they
are not able to produce price. We filter those rows from the dataset.

```{r transform1, echo=TRUE}
mydata$Date_Id <- ymd(mydata$Date_Id) # transform to date format
mydata$sales <- mydata$valuesales * 1000 # transform to currency
mydata <- mydata %>% filter(volumesales > 0)
mydata$price <- round(mydata$sales / mydata$volumesales, 4) # find price / litre
mydata$volume <- mydata$volumesales # name litres correctly for simplicity
mydata$SalesItem <- as.numeric(as.character(mydata$SalesItem))
mydata <- mydata %>% select(Date_Id, Shop_Id, SKU_Id, sales, price, volume)
dim(mydata)
```

There are rows with `NULL` sales. Those are not useful as well. We filter them
from the dataset. This reduces the dataset to **2.4M** rows.

```{r transform2, echo=TRUE}
mydata <- mydata %>% filter(sales > 0)
mydata <- mydata %>% filter(price > 0)
dim(mydata)
```

### Tidying data

There are three types of outliers in the dataset. They refer to sales, price and
volume. Outliers are located in different rows across those columns.

```{r data_summary}
summary(mydata)
```

We tidy the data from outliers starting from sales. Sales outliers are hard to
see in a histogram. The only evidence of outliers is the unusually wide limits
of the x-axis.

```{r plot1}
ggplot(mydata) + geom_density(aes(sales))
```

To make it easier to see the unusual values, we zoom in to rare values of sales
with `coord_cartesian` in `ggplot` library. There are rare weeks with sales 
over **100000** - those are outliers.

```{r plot2, echo=TRUE}
ggplot(mydata) + 
    geom_histogram(aes(sales), bins = 20, fill = "steelblue") +
    coord_cartesian(ylim = c(0, 100))
```

We use univariate approach do identify the outliers for sales. This approach
uses interquantile range of a distribution (IQR) to find outliers. Usual limits
calculation involves **1.5** IQR.

```{r sales_outliers1, echo=TRUE}
qnt <- quantile(mydata$sales, probs = c(0.25, 0.75), na.rm = TRUE)
h <- 1.5 * (qnt[2] - qnt[1])
lower_limit <- qnt[1] - h
upper_limit <- qnt[2] + h
c(lower_limit, upper_limit)
```

We replace observations behind the limits with `NA` values. Since they have 
minimal effect on *Mean* and *Median* and we do not know why they are there,
we will drop them if their count is less than 10% of population.

```{r sales_outliers2, echo=TRUE}
mydata$sales[mydata$sales < lower_limit] <- NA
mydata$sales[mydata$sales > upper_limit] <- NA
round(sum(is.na(mydata$sales)) / nrow(mydata), 3)
```

Since the share of `NA` values is **1%** of population, we omit those values.

```{r omit_sales_outliers}
mydata <- mydata %>% filter(!is.na(sales))
```

Now the distribution of sales looks more reliable.

```{r plot3}
ggplot(mydata) + geom_histogram(aes(sales), bins = 15, fill = "steelblue")
```

Repeat outliers removal for the price column.

```{r price_outliers, echo=TRUE}
qnt <- quantile(mydata$price, probs = c(0.25, 0.75), na.rm = TRUE)
h <- 1.5 * (qnt[2] - qnt[1])
lower_limit <- qnt[1] - h
upper_limit <- qnt[2] + h
mydata$price[mydata$price < lower_limit] <- NA
mydata$price[mydata$price > upper_limit] <- NA
mydata <- mydata %>% filter(!is.na(price))
ggplot(mydata) + geom_histogram(aes(price), bins = 15, fill = "steelblue")
```

And finally, the volume column.

```{r volume_outliers, echo=TRUE}
qnt <- quantile(mydata$volume, probs = c(0.25, 0.75), na.rm = TRUE)
h <- 1.5 * (qnt[2] - qnt[1])
lower_limit <- qnt[1] - h
upper_limit <- qnt[2] + h
mydata$volume[mydata$volume < lower_limit] <- NA
mydata$volume[mydata$volume > upper_limit] <- NA
mydata <- mydata %>% filter(!is.na(volume))
ggplot(mydata) + geom_histogram(aes(volume), bins = 15, fill = "steelblue")
```

Now the summary shows that the data is more applicable to further analysis.

```{r data_summary2}
summary(mydata)
```









